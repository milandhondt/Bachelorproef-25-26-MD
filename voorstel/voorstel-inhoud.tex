%===================
%    INLEIDING
%===================
\section{Inleiding}%
\label{sec:inleiding}

De afgelopen jaren hebben Large Language Models (LLM's) zoals ChatGPT, Gemini, Claude, \newline DeepSeek en GitHub Copilot een enorme impact gecreëerd binnen alle sectoren en domeinen. Artificiële Intelligentie (AI) valt bij velen niet meer weg te denken uit het dagelijks leven, en wordt elke dag opnieuw door ontelbare aantallen mensen ingezet om hun taken te vereenvoudigen. Zo ook door studenten Full Stack Development binnen de opleiding Toegepaste Informatica aan HOGENT. Deze studenten zetten LLM's steeds vaker in om code te begrijpen, debuggen, zelfs schrijven.

\vspace{1em}

Maar hoe goed is de feedback die ze genereren? Is deze wel correct? Is deze bruikbaar? Dat zijn allemaal vragen waar de studenten zelf weinig bij stil staan, maar dit is wel relevant. Binnen de opleidingsonderdelen Front-End Web Development en Web Services worden er namelijks per jaar gemiddeld 200 projecten ingediend. Deze worden momenteel nog volledig handmatig beoordeeld door de lectoren. Die manuele beoordeling is enorm tijdsintensief en repetitief. Lectoren moeten dezelfde criteria, denk bijvoorbeeld aan structuur, consistentie, documentatie, foutafhandeling, testing,\ldots telkens opnieuw toepassen op elk project. Hierdoor gaat eigenlijk een groot deel van hun tijd naar het puur controleren van de aanwezigheid van bepaalde onderdelen in het project, en niet naar het geven van inhoudelijke feedback. Verder is het ook lastig voor de lectoren om consistent feedback te geven. Zo hangt de feedback die een bepaald project zou krijgen ook nog eens af van welke persoon het beoordeelt. Zo kan vermoeidheid er bijvoorbeeld voor zorgen dat een lector die al vele projecten heeft nagekeken minder fouten opmerkt dan een collega die met hetzelfde project hun dag zou starten.

\vspace{1em}

Deze concrete probleemsituatie geeft aanleiding tot de vraag of lectoren LLM's zouden kunnen gebruiken ter ondersteuning, door feedback te genereren over de code-kwaliteit van een studentenproject. Hoe bruikbaar, betrouwbaar, consistent of accuraat deze feedback is, en hoe consistent ze is in vergelijking met de feedback van een lector is echter nog onduidelijk.

\vspace{1em}

Daaruit vloeit dus ook volgende onderzoeksvraag voort:

\begin{quote}
    \textit{``In welke mate kunnen Large Language Models bruikbare en betrouwbare formatieve feedback genereren op code-kwaliteit van studentenprojecten binnen webontwikkeling en hoe verhouden deze beoordelingen zich tot de evaluaties van ervaren lectoren?''}
\end{quote}

\vspace{1em}

Om deze onderzoeksvraag te beantwoorden worden onderstaande deelvragen onderzocht. \newline Deze hebben enerzijds betrekking tot het probleemdomein, anderzijds tot het oplossingsdomein.

\vspace{1em}

\textbf{\newline Deelvragen probleemdomein:}
\begin{enumerate}
    \item Welke specifieke evaluatiecriteria worden momenteel gebruikt bij de evaluatie van projecten binnen Front-end Web Development en Web Services?
    \item Hoeveel tijd wordt gemiddeld gespendeerd aan het beoordelen van 1 project?
    \item Van de tijd die gespendeerd wordt aan het beoordelen van één project, welk deel is gericht op het controleren van de criteria, en welk deel is gericht op het geven van inhoudelijke feedback?
    \item In welke mate ervaren de lectoren inconsistenties in hun eigen beoordelingen ten opzichte van hun collega's?
\end{enumerate}

\vspace{1em}

\textbf{Deelvragen oplossingsdomein:}
\begin{enumerate}
    \item In welke mate zijn verschillende LLM’s in staat om vooraf gedefinieerde evaluatiecriteria correct te detecteren in de codebase van een studentenproject?
    \item In welke mate komt de door LLM’s gegenereerde feedback overeen met de feedback van lectoren?
    \item Hoe consistent is de feedback van verschillende LLM's bij hetzelfde project?
    \item Op welke criteria scoren LLM’s sterk en op welke scoren ze zwakker bij het evalueren van studentenprojecten?
    \item Wat is de potentiële tijdswinst bij het gebruik van een LLM, ten opzichte van alle projecten handmatig evalueren?
    \item Welke prompt-engineeringtechnieken verhogen de kwaliteit en betrouwbaarheid van LLM-gebaseerde code-evaluatie?
\end{enumerate}

\vspace{1em}

Het doel van dit onderzoek is dus om te gaan kijken in hoeverre LLM's gebruikt kunnen worden ter ondersteuning bij het evalueren van studentenprojecten. Naast de scriptie omvat het eindresultaat ook een Proof-Of-Concept (POC) testomgeving waarmee LLM's systematisch beoordeeld zullen worden op o.a. hun vermogen om criteria te herkennen en consistente feedback te genereren. De doelgroep van dit onderzoek bestaat uit de lectoren Front-End Web Development en Web Services aan HOGENT.

\section{Literatuurstudie}%
\label{sec:literatuurstudie}

De snelle opkomst van LLM's heeft het potentieel om verschillende aspecten van het programmeeronderwijs drastisch te veranderen. Waar vroeger vooral gebruik gemaakt werd van autograding, kunnen nu LLM's worden ingezet om meer kwalitatieve en contextbewuste feedback te genereren. Binnen deze literatuurstudie wordt gekeken naar autograding, maar ook naar recente onderzoeken waarbij AI gebruikt werd voor het evalueren van code en het ondersteunen van studenten. Daarnaast wordt er bekeken welke impact en risico's AI met zich meebrengt, en welke beperkingen LLM's hebben.

\paragraph{De evolutie van automatische beoordelingen binnen het programmeeronderwijs}
\leavevmode\par\noindent
Automatische beoordelingen, ook wel gekend als autograding, wordt steeds belangrijker binnen programmeeronderwijs. Deze systemen richten zich voornamelijk op correcte syntax en de resultaten bij het uitvoeren van de code, waardoor de focus meer op kwantitatieve beoordeling ligt, in de plaats van kwalitatieve feedback geven \autocite{Messer2024}. Hoewel deze systemen de docenten dus wel ontlasten, blijven ze echter beperkt in het bieden van gedetailleerde uitleg, en zijn ze ook minder goed in het detecteren van creatievere of alternatieve oplossingen. 

\paragraph{De opkomst van LLM's binnen het onderwijs}
De introductie van LLM's zoals ChatGPT en Gemini biedt dan weer veel meer mogelijkheden voor het automatiseren van feedback en beoordelingen. \textcite{Ye2023} waarschuwt echter voor hallucinaties, waarbij een model foutieve of inconsistente feedback genereert. Daarom benadrukt \textcite{Kasneci2023} dat LLM-gebruik binnen onderwijsomgevingen altijd gepaard moet gaan met duidelijke pedagogische richtlijnen. Zo moeten de studenten zeer goed begrijpen dat LLM's beperkingen hebben, en dat de feedback niet altijd correct of volledig is. Met een kritische blik kijken naar de LLM output blijft cruciaal, alsook de controle van lectoren op die output. Als laatste zou LLM-ondersteuning geleidelijk moeten worden afgebouwd naarmate studenten meer kennis opbouwen, om te voorkomen dat zij er te afhankelijk van worden.


\paragraph{De opkomst van LLM's binnen het onderwijs}
De introductie van LLM's zoals ChatGPT en Gemini biedt dan weer veel meer mogelijkheden voor het automatiseren van feedback en beoordelingen. Niet alleen kan een LLM syntaxfouten detecteren, het kan ook de context van een bepaalde implementatie begrijpen en hier suggesties of verbeteringen voor geven, iets wat traditionele autograding tools niet kunnen \autocite{Kasneci2023}. \textcite{Ye2023} waarschuwt echter voor hallucinaties, waarbij een model foutieve of inconsistente feedback genereert. Daarom benadrukt \textcite{Kasneci2023} dat LLM-gebruik binnen onderwijsomgevingen altijd gepaard moet gaan met duidelijke pedagogische richtlijnen. Zo moeten studenten goed begrijpen dat LLM's beperkingen hebben en dat de feedback niet altijd correct of volledig is. Met een kritische blik kijken naar de output van LLM’s blijft cruciaal, evenals de controle door lectoren.


\paragraph{LLM's voor het automatisch genereren van feedback en beoordelen van code-kwaliteit}
\leavevmode\par\noindent
Recent onderzoek toont aan dat het verschil tussen LLM's en menselijke feedback steeds kleiner wordt. Zo vergelijkt \textcite{Mendonca2025} een open-source model (LLaMA 3.2) met een betalend model (GPT-4o) en mensen. Uit het onderzoek bleek dat het betalend model praktisch dezelfde scores levert voor vragen over code. Ook \textcite{Koutcheme2025} onderzocht open-source LLM's, en toonde aan dat deze efficiënt en bijna even accuraat feedback kunnen genereren en beoordelen als eigen modellen. Verder benadrukte \textcite{Adurean2025} dat fine-tuning samen met student-gegenereerde feedback de stijl en precisie van AI gegenereerde feedback verder verbetert. Hierdoor komt deze nog dichter bij menselijke evaluaties.

\paragraph{De impact op studenten en hun leerproces}
Het gebruiken van door LLM gegenereerde feedback heeft invloed op zowel de prestaties als het doorzettingsvermogen van studenten. \textcite{Zhou2025} toont in een Randomized Controlled Trial (RCT) aan dat door LLM gegenereerde feedback de kans zou vergroten dat studenten problemen oplossen, dat ze hogere scores zouden behalen en dat ze minder onproductief ``wheel-spinning'' vertonen, een term die verwijst naar het blijven proberen zonder effectieve vooruitgang te boeken. Deze effecten waren zichtbaar bij zowel eenvoudige als complexe programmeertaken, en verdwenen van zodra de feedback werd verwijderd. \textcite{Lepp2025} onderzocht dan weer de studentenpercepties, en rapporteerde dat studenten LLM's vooral gebruiken voor het begrijpen van code, en voor het oplossen van problemen. Dit heeft een positieve invloed op hun leerervaring, maar uiteraard vormt overmatige afhankelijkheid van AI wel een risico.

\paragraph{De uitdagingen die gepaard gaan met LLM's}
Ondanks de vele voordelen brengen LLM's ook nadelen en uitdagingen met zich mee. Zo benadrukt \textcite{Ye2023} de aanwezigheid van hallucinaties en een bias in LLM-output. \textcite{Gotoman2025} onderzocht dan weer detectietools voor teksten gegenereerd door AI en concludeerde dat de betrouwbaarheid van deze tools nog variabel is. Menselijk toezicht blijft essentieel. Als laatste benadrukte \textcite{Kasneci2023} dat de combinatie van pedagogische strategieën en kennis van AI noodzakelijk is om zowel studenten als docenten bewust te maken van de beperkingen.

\paragraph{Onderzoeksrelevantie}
\leavevmode\par\noindent
Hoewel LLM's steeds vaker onderzocht worden, ook binnen programmeeronderwijs, is het duidelijk dat er verder onderzoek moet worden gedaan naar hoe lectoren LLM's efficiënt kunnen inzetten ter ondersteuning bij het beoordelen van studentenprojecten. Bestaande studies richten zich namelijk vooral op autograding voor individuele programmeer oefeningen. Daardoor blijft het onduidelijk in hoeverre door LLM gegenereerde feedback bij kan dragen aan consistente evaluaties, de werkdruk en efficiëntie van lectoren kan verbeteren, zeker binnen zeer uiteenlopende studentenprojecten. Het verder verkennen van deze toepassing is essentieel om te bepalen of LLM's daadwerkelijk ingezet kunnen worden voor deze doeleinden.

\section{Methodologie}%
\label{sec:methodologie}

%===================
%    INLEIDENDE 
%      ALINEA
%===================
Deze bachelorproef zal uitgewerkt worden in verschillende fases. In elk van deze fases wordt een andere techniek gebruikt, en wordt de focus op een ander onderdeel van het probleem gelegd. In totaal zijn er 14 werkdagen voorzien voor de bachelorproef. Aangezien er 1 werkdag per week besteed wordt aan het onderzoek, komt 1 werkdag overeen met 1 week.

Een overzicht van de fasen wordt weergegeven in Figuur~\ref{fig:gantt}.

%===================
%      FASE 1
%===================
\paragraph{Fase 1: Probleemdomein onderzoeken (1 werkdag)}
\leavevmode\par\noindent
In de eerste fase van dit onderzoek zal het probleemdomein verder worden ontdekt. Hiervoor zal er een interview worden afgenomen met een lector Front-end Web Development \& Web Services. Dit is namelijk de primaire doelgroep van deze bachelorproef. Dit interview wordt gedaan voor 3 redenen: 
\begin{itemize}
    \item Het verder verdiepen in het probleem en de evaluatiecriteria en methoden die de lectoren vandaag hanteren.
    \item De nood aan onderzoek, en de relevantie van het onderzoek verder te verduidelijken.
    \item Nagaan of er studentenprojecten van verschillende kwaliteitsniveaus kunnen worden opgeleverd. Dit zou bij voorkeur ook door verschillende lectoren gedaan worden, om de consistentie van de evaluaties in acht te kunnen nemen.
\end{itemize}

\vspace{1em}

Het resultaat van deze fase is een duidelijker inzicht binnen het probleemdomein, een lijst met de verschillende evaluatiecriteria, alsook verschillende studentenprojecten die gebruikt kunnen worden als testprojecten in een volgende fase.

%===================
%      FASE 2
%===================
\paragraph{Fase 2: Literatuurstudie (2 werkdagen)}
\leavevmode\par\noindent
De tweede fase bevat de literatuurstudie. Hierin zal er gekeken worden naar welke LLMs (Large Languague Models) er het meest geschikt zijn voor het analyseren van code-kwaliteit, en het geven van consistente en duidelijke feedback. Deze literatuurstudie zal zowel lokale als cloud modellen onderzoeken, via Ollama. Hieruit zal een longlist worden opgesteld met de potentiële modellen die in aanmerking komen voor verder onderzoek. Bij het kiezen van de modellen wordt er aandacht besteed aan:
\begin{itemize}
    \item Voordelen
    \item Nadelen
    \item Token- en contextlimitaties
    \item Geschiktheid voor code-analyse
\end{itemize}

Verder zal er ook gekeken worden naar het potentieel van MastraAI. Dit is een AI-platform dat tools voor code-intelligentie en geautomatiseerde code-analyse biedt, inclusief suggesties voor verbetering en kwaliteitsbewaking. Daarnaast wordt er aandacht besteed aan het zoeken naar een oplossing voor het opsplitsen van te grote projecten. Door token- en contextlimitaties kan het namelijk voorkomen dat volledige projecten niet in één prompt kunnen worden beoordeeld. Voor deze opsplitsing bestaan er verschillende manieren, zoals per bestand, per laag, via chunking, via een samenvatting,\ldots

\vspace{1em}

Het resultaat van deze fase is een duidelijke, goed onderbouwde lijst van bruikbare modellen, inclusief hun technische eigenschappen en eventuele beperkingen.

%===================
%      FASE 3
%===================
\paragraph{Fase 3: Ontwerp en opzetten testomgeving en PoC (4 werkdagen)} 
\leavevmode\par\noindent
De derde fase wordt opgesplitst in meerdere subfasen om zowel het ontwerp, als de uitwerking van de testomgeving, iteratief en gestructureerd te laten verlopen. De testomgeving moet namelijk herbruikbaar, reproduceerbaar en geschikt zijn voor meerdere evaluatierondes. De testomgeving omvat alles van tools, scripts en configuraties waar het onderzoek mee zal worden uitgevoerd. De PoC omvat het gehele evaluatieproces en draait binnen deze testomgeving. Hierbij is het nog belangrijk om te vermelden dat het de PoC is die iteratief zal worden bijgewerkt, en niet de volledige testomgeving.

\subparagraph{3.1 Ontwerp PoC} 
\leavevmode\par\noindent
In deze eerste subfase wordt de volledige PoC eerst conceptueel uitgewerkt. Hieronder valt als eerste het uitwerken van de promptstructuur en de gewenste vorm van output. Ook wordt er gekeken welke evaluatiecriteria in welke volgorde getest zullen worden. Zo kan er in een eerste iteratie gewerkt worden met een beperktere set criteria, bijvoorbeeld enkel database- of repositorycriteria. Dan zal er ook nog een plan worden opgesteld dat zal beschrijven hoe de PoC bij elke uitbreiding wordt bijgestuurd. Als laatste zal er gekeken worden naar welke LLM's gebruikt zullen worden in de testomgeving, en zal ook een plan opgesteld worden voor de iteratieve evaluaties

\subparagraph{3.2 Opzetten eerste versie testomgeving} 
\leavevmode\par\noindent
In deze fase zal de eerste versie van de testomgeving worden opgezet. Hiervoor wordt gekeken naar de zaken die opgesteld werden in vorige subfase. Wat ook belangrijk is om te vermelden, is dat in deze fase maar 1 kleine set criteria zal bekeken worden. Zo kan er namelijk nog makkelijk bijgestuurd worden als er ergens iets in de testomgeving nog niet helemaal juist zit. Tenslotte worden in deze fase de scripts opgesteld die de studentenprojecten automatisch opsplitsen om binnen de tokenlimieten te blijven, en die de opgesplitste delen met een identieke prompt automatisch naar de geselecteerde modellen versturen.

\subparagraph{3.3 Iteratieve uitbreiding per criteriaset}
\leavevmode\par\noindent
In deze fase wordt de PoC binnen de stabiel opgezette testomgeving iteratief uitgebreid. Elke iteratie zal nieuwe criteria bevatten, alsook eventuele aanpassingen of uitbreidingen aan prompts om de LLM's te helpen bij het evalueren van deze criteria. Na het toevoegen zullen alle modellen opnieuw worden geëvalueerd. Deze evaluatie gebeurt door te analyseren in welke mate de gegenereerde feedback consistent is, in welke mate de antwoorden overeenkomen met vooraf gedefinieerde verwachtingen en welke inconsistenties erop duiden dat verdere tweaks nodig zijn. Hierna kunnen er eventueel terug nodige aanpassingen gedaan worden aan de PoC, waarna een nieuwe iteratie van start kan gaan. Dit proces zal vervolgens herhaald blijven worden, totdat alles tot op een bepaald punt geoptimaliseerd is.

\vspace{1em}

Het resultaat van de derde fase is:
\begin{itemize}
    \item een volledig gedocumenteerde, reproduceerbare en herbruikbare PoC-testomgeving;
    \item meerdere iteraties van verbeterde prompts en evaluaties;
    \item een GitHub-repository met het ontwerp, de scripts, documentatie, logging, configuratiebestanden en testresultaten.
\end{itemize}

%===================
%      FASE 4
%===================
\paragraph{Fase 4: Kwantitatieve evaluatie van LLMs (5 werkdagen)}
\leavevmode\par\noindent 
De vierde fase zal het individueel evalueren van verschillende LLMs omvatten. Op basis van de literatuurstudie zullen de meest relevante en bruikbare modellen worden gekozen, en deze zullen individueel worden beoordeeld. Naast de inhoudelijke kwaliteit van de gegenereerde feedback, zal er ook gekeken worden naar performance-gerelateerde aspecten. Hierbij worden volgende criteria onderzocht:
\begin{itemize}
    \item Klopt de output?
    \item Is de output consistent met die van lectoren?
    \item Is de output consistent wanneer hetzelfde project meerdere keren geëvalueerd wordt?
    \item Hoeveel potentiële tijdswinst zit er verbonden aan LLM ondersteunde code-evaluatie?
    \item Hoeveel tijd, CPU/GPU-gebruik en geheugen verbruiken de verschillende modellen tijdens het evalueren?
    \item Hoe verhoudt de performantie van een model zich ten opzichte van de kwaliteit van de output?
\end{itemize}

Wanneer alle modellen individueel zijn beoordeeld, dan volgt de vergelijkende studie. Hierin worden de verschillende modellen met elkaar vergeleken op alle bovenstaande criteria.

\vspace{1em}

Het resultaat van deze fase is een uitgebreide analyse en een grondige vergelijking op basis van de bekomen resultaten.

%===================
%      FASE 5
%===================
\paragraph{Fase 5: Conclusie (2 werkdagen)}
\leavevmode\par\noindent
In de laatste fase zal de conclusie worden opgebouwd. De resultaten van al het onderzoek uit voorgaande fases worden gebundeld om objectief aan te tonen of LLMs mogelijks gebruikt kunnen worden voor het evalueren van studentenprojecten binnen het afgebakende probleemdomein. Daarnaast worden de verschillende voor- en nadelen van de geëvalueerde modellen toegelicht, om de keuze voor lectoren te vergemakkelijken.

\vspace{1em}

Het resultaat van deze fase is een volledig uitgewerkte conclusie, aangevuld met de bevindingen, aanbevelingen voor lectoren en eventuele suggesties voor verder onderzoek.

\section{Verwacht resultaat, conclusie}%
\label{sec:verwachte_resultaten}

Op basis van de vooraf geplande POC testomgeving en de vergelijkingen tussen de feedback van lectoren en LLM's wordt er verwacht dat het onderzoek geen simpele oplossing zal opleveren. Niet 1 LLM zal de allerbeste zijn. De gebruikte LLM's zullen waarschijnlijk sterk presteren in het herkennen van duidelijke criteria zoals documentatie, structuur, foutafhandeling en de aanwezigheid van testen. Voor meer inhoudelijke of contextgebonden feedback wordt er minder consistentie verwacht. De opmerkingen van LLM's hier zullen waarschijnlijk algemener zijn, en beperkt toepasbaar. Daarom zou het interessant kunnen zijn om, in een latere fase, meerdere LLM's te laten samenwerken. Door bijvoorbeeld modellen parallel te laten evalueren, of door aparte modellen te gaan gebruiken per onderdeel van de evaluatie. Op deze manier zou de feedback gevarieerder en meer betrouwbaar kunnen zijn, dan wanneer er slechts één model wordt gebruikt

\vspace{1em}

Hiernaast wordt er ook vermoed dat de feedback van 1 bepaald model relatief consistent zou blijven over meerdere analyses van eenzelfde project. Tussen verschillende modellen zullen er naar verwachting wel merkbare verschillen naar boven komen. Denk aan niveau van detail, schrijfstijl, correctheid en strengheid. In de vergelijking met de feedback van lectoren zal vermoedelijk blijken dat LLM's de controles op basiscriteria sneller kunnen uitvoeren, maar dat ze nog niet in staat zijn om de complexere projecten volledig accuraat en betrouwbaar te beoordelen.

\vspace{1em}

De potentiële tijdswinst vormt voor de lectoren waarschijnlijk een van de grootste meerwaarden. Om verschillende basis controles, die voor elk project hetzelfde zijn, steeds opnieuw te moeten uitvoeren is zeer repetitief. Als deze automatisch zouden kunnen verlopen, ontstaat er voor hun meer ruimte voor inhoudelijke feedback. Dit betekent niet dat LLM's in staat zijn om een volledig project te gaan beoordelen, maar wel dat ze als ondersteuning toch een deel van de werkdruk aanzienlijk kunnen verlagen. Uiteraard moet de LLM gegenereerde feedback dan wel nog steeds worden nagekeken.

\vspace{1em}

Over het algemeen wordt er verwacht dat dit onderzoek zal aantonen dat LLM's bruikbaar zijn ter ondersteuning binnen de evaluatie van studentenprojecten voor de vakken Front-End Web Development en Web Services aan HOGENT. Voornamelijk het detecteren van aanwezige criteria binnen het project lijkt handig en voor een enorme tijdswinst te zorgen voor de lectoren. De uiteindelijke inhoudelijke evaluatie blijft waarschijnlijk best nog even bij de lectoren zelf, omdat LLM's hier meer moeite mee hebben. In hoeverre LLM's zullen evolueren de komende tijd is echter onbekend, waardoor dit in de toekomst zeker nog zou kunnen veranderen.