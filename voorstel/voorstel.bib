% Encoding: UTF-8

@Article{Mendonca2025,
  author         = {Mendonça, Pedro C. and Quintal, Filipe and Mendonça, Fábio},
  date           = {2025},
  journaltitle   = {Applied Sciences},
  title          = {Evaluating LLMs for Automated Scoring in Formative Assessments},
  doi            = {10.3390/app15052787},
  issn           = {2076-3417},
  number         = {5},
  url            = {https://www.mdpi.com/2076-3417/15/5/2787},
  urldate        = {2025-11-30},
  volume         = {15},
  abstract       = {The increasing complexity and scale of modern education have revealed the shortcomings of traditional grading methods in providing consistent and scalable assessments. Advancements in artificial intelligence have positioned Large Language Models (LLMs) as robust solutions for automating grading tasks. This study systematically compared the grading performance of an open-source LLM (LLaMA 3.2) and a premium LLM (OpenAI GPT-4o) against human evaluators across diverse question types in the context of a computer programming subject. Using detailed rubrics, the study assessed the alignment between LLM-generated and human-assigned grades. Results revealed that while both LLMs align closely with human grading, equivalence testing demonstrated that the premium LLM achieves statistically and practically similar grading patterns, particularly for code-based questions, suggesting its potential as a reliable tool for educational assessments. These findings underscore the ability of LLMs to enhance grading consistency, reduce educator workload, and address scalability challenges in programming-focused assessments.},
  article-number = {2787},
  file           = {:Evaluating LLMs for Automated Scoring in Formative Assessments.pdf:PDF},
  priority       = {prio1},
}

@Article{Kasneci2023,
  author       = {Enkelejda Kasneci and Kathrin Sessler and Stefan Küchemann and Maria Bannert and Daryna Dementieva and Frank Fischer and Urs Gasser and Georg Groh and Stephan Günnemann and Eyke Hüllermeier and Stephan Krusche and Gitta Kutyniok and Tilman Michaeli and Claudia Nerdel and Jürgen Pfeffer and Oleksandra Poquet and Michael Sailer and Albrecht Schmidt and Tina Seidel and Matthias Stadler and Jochen Weller and Jochen Kuhn and Gjergji Kasneci},
  date         = {2023},
  journaltitle = {Learning and Individual Differences},
  title        = {ChatGPT for good? On opportunities and challenges of large language models for education},
  doi          = {https://doi.org/10.1016/j.lindif.2023.102274},
  issn         = {1041-6080},
  pages        = {102274},
  url          = {https://www.sciencedirect.com/science/article/pii/S1041608023000195},
  urldate      = {2025-11-30},
  volume       = {103},
  abstract     = {Large language models represent a significant advancement in the field of AI. The underlying technology is key to further innovations and, despite critical views and even bans within communities and regions, large language models are here to stay. This commentary presents the potential benefits and challenges of educational applications of large language models, from student and teacher perspectives. We briefly discuss the current state of large language models and their applications. We then highlight how these models can be used to create educational content, improve student engagement and interaction, and personalize learning experiences. With regard to challenges, we argue that large language models in education require teachers and learners to develop sets of competencies and literacies necessary to both understand the technology as well as their limitations and unexpected brittleness of such systems. In addition, a clear strategy within educational systems and a clear pedagogical approach with a strong focus on critical thinking and strategies for fact checking are required to integrate and take full advantage of large language models in learning settings and teaching curricula. Other challenges such as the potential bias in the output, the need for continuous human oversight, and the potential for misuse are not unique to the application of AI in education. But we believe that, if handled sensibly, these challenges can offer insights and opportunities in education scenarios to acquaint students early on with potential societal biases, criticalities, and risks of AI applications. We conclude with recommendations for how to address these challenges and ensure that such models are used in a responsible and ethical manner in education.},
  file         = {:ChatGPT-for-good-On-opportunities-and-challenges-of-large-language-models-for-education.pdf:PDF},
  keywords     = {Large language models, Artificial intelligence, Education, Educational technologies},
  priority     = {prio1},
}

@Article{Ye2023,
  author   = {Hongbin Ye and Tong Liu and Aijia Zhang and Wei Hua and Weiqiang Jia},
  date     = {2023-09-13},
  title    = {Cognitive Mirage: A Review of Hallucinations in Large Language Models},
  doi      = {https://doi.org/10.48550/arXiv.2309.06794},
  url      = {https://arxiv.org/abs/2309.06794},
  urldate  = {2025-11-30},
  file     = {:C\:/Users/milan/Downloads/Cognitive Mirage A Review of Hallucinations in Large Language Models.pdf:PDF},
  priority = {prio1},
}

@Article{Sadik2008,
  author       = {Sadik, Alaa},
  date         = {2008-04},
  journaltitle = {Educational Technology Research and Development},
  title        = {Digital storytelling: a meaningful technology-integrated approach for engaged student learning},
  doi          = {10.1007/s11423-008-9091-8},
  issn         = {1556-6501},
  number       = {4},
  pages        = {487--506},
  url          = {https://link.springer.com/article/10.1007/s11423-008-9091-8},
  urldate      = {2025-11-30},
  volume       = {56},
  file         = {:Digital storytelling a meaningful technology-integrated approach for engaged student learning.pdf:PDF},
  priority     = {prio1},
  publisher    = {Springer Science and Business Media LLC},
}

@InProceedings{Sarsa2022,
  author     = {Sarsa, Sami and Denny, Paul and Hellas, Arto and Leinonen, Juho},
  booktitle  = {Proceedings of the 2022 ACM Conference on International Computing Education Research - Volume 1},
  date       = {2022-08},
  title      = {Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models},
  doi        = {10.1145/3501385.3543957},
  pages      = {27--43},
  publisher  = {ACM},
  series     = {ICER 2022},
  url        = {https://dl.acm.org/doi/10.1145/3501385.3543957},
  urldate    = {2025-11-30},
  collection = {ICER 2022},
  file       = {:Automatic generation of programming exercises and code explanations using large language models.pdf:PDF},
  priority   = {prio1},
}

@Article{Messer2024,
  author       = {Messer, Marcus and Brown, Neil C. C. and Kölling, Michael and Shi, Miaojing},
  date         = {2024-02},
  journaltitle = {ACM Transactions on Computing Education},
  title        = {Automated Grading and Feedback Tools for Programming Education: A Systematic Review},
  doi          = {10.1145/3636515},
  issn         = {1946-6226},
  number       = {1},
  pages        = {1--43},
  url          = {https://dl.acm.org/doi/10.1145/3636515},
  urldate      = {2025-11-30},
  volume       = {24},
  file         = {:Automated Grading and Feedback Tools for Programming.pdf:PDF},
  priority     = {prio1},
  publisher    = {Association for Computing Machinery (ACM)},
}

@Misc{Zhou2025,
  author    = {Zhou, Yiqiu and Pankiewicz, Maciej and Paquette, Luc and Baker, Ryan},
  date      = {2025},
  editor    = {Jiang and B. et al},
  title     = {Impact of LLM Feedback on Learner Persistence in Programming},
  url       = {https://learninganalytics.upenn.edu/ryanbaker/ICCE2025_paper_28.pdf},
  urldate   = {2025-11-30},
  abstract  = {This study examines how Large Language Model (LLM) feedback generated for compiler errors impacts learners' persistence in programming tasks within a system for automated assessment of programming assignments. Persistence, the ability to maintain effort in the face of challenges, is crucial for academic success but can sometimes lead to unproductive "wheel spinning" when students struggle without progress. We investigated how additional LLM feedback based on the GPT-4 model, provided for compiler errors affects learners' persistence within a CS1 course. Specifically, we examined whether its impacts differ based on task difficulty, and if the effects persist after the feedback is removed. A randomized controlled trial involving 257 students across various programming tasks was conducted. Our findings reveal that LLM feedback improved some aspects of students' performance and persistence, such as increased scores, a higher likelihood of solving problems, and a lower tendency to demonstrate unproductive "wheel spinning" behavior. Notably, this positive impact was also observed in challenging tasks. However, its benefits did not sustain once the feedback was removed. The results highlight both the potential and limitations of LLM feedback, pointing out the need to promote long-term skill development and learning independent of immediate AI assistance.},
  booktitle = {Jiang, B. et al. (Eds.) (2025). Proceedings of the 33rd International Conference on Computers in Education.},
  file      = {:Impact of LLM Feedback on Learner  Persistence in Programming.pdf:PDF},
  keywords  = {Autograding, Automated feedback, Programming, Persistence, GPT, LLM},
  priority  = {prio1},
  publisher = {Springer},
  year      = {2025},
}

@Comment{jabref-meta: databaseType:biblatex;}
