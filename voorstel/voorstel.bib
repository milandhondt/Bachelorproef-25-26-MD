% Encoding: UTF-8

@Article{Mendonca2025,
  author         = {Mendonça, Pedro C. and Quintal, Filipe and Mendonça, Fábio},
  date           = {2025},
  journaltitle   = {Applied Sciences},
  title          = {Evaluating LLMs for Automated Scoring in Formative Assessments},
  doi            = {10.3390/app15052787},
  issn           = {2076-3417},
  number         = {5},
  url            = {https://www.mdpi.com/2076-3417/15/5/2787},
  urldate        = {2025-11-30},
  volume         = {15},
  abstract       = {The increasing complexity and scale of modern education have revealed the shortcomings of traditional grading methods in providing consistent and scalable assessments. Advancements in artificial intelligence have positioned Large Language Models (LLMs) as robust solutions for automating grading tasks. This study systematically compared the grading performance of an open-source LLM (LLaMA 3.2) and a premium LLM (OpenAI GPT-4o) against human evaluators across diverse question types in the context of a computer programming subject. Using detailed rubrics, the study assessed the alignment between LLM-generated and human-assigned grades. Results revealed that while both LLMs align closely with human grading, equivalence testing demonstrated that the premium LLM achieves statistically and practically similar grading patterns, particularly for code-based questions, suggesting its potential as a reliable tool for educational assessments. These findings underscore the ability of LLMs to enhance grading consistency, reduce educator workload, and address scalability challenges in programming-focused assessments.},
  article-number = {2787},
  file           = {:Evaluating LLMs for Automated Scoring in Formative Assessments.pdf:PDF},
  priority       = {prio1},
}

@Article{Kasneci2023,
  author       = {Enkelejda Kasneci and Kathrin Sessler and Stefan Küchemann and Maria Bannert and Daryna Dementieva and Frank Fischer and Urs Gasser and Georg Groh and Stephan Günnemann and Eyke Hüllermeier and Stephan Krusche and Gitta Kutyniok and Tilman Michaeli and Claudia Nerdel and Jürgen Pfeffer and Oleksandra Poquet and Michael Sailer and Albrecht Schmidt and Tina Seidel and Matthias Stadler and Jochen Weller and Jochen Kuhn and Gjergji Kasneci},
  date         = {2023},
  journaltitle = {Learning and Individual Differences},
  title        = {ChatGPT for good? On opportunities and challenges of large language models for education},
  doi          = {https://doi.org/10.1016/j.lindif.2023.102274},
  issn         = {1041-6080},
  pages        = {102274},
  url          = {https://www.sciencedirect.com/science/article/pii/S1041608023000195},
  urldate      = {2025-11-30},
  volume       = {103},
  abstract     = {Large language models represent a significant advancement in the field of AI. The underlying technology is key to further innovations and, despite critical views and even bans within communities and regions, large language models are here to stay. This commentary presents the potential benefits and challenges of educational applications of large language models, from student and teacher perspectives. We briefly discuss the current state of large language models and their applications. We then highlight how these models can be used to create educational content, improve student engagement and interaction, and personalize learning experiences. With regard to challenges, we argue that large language models in education require teachers and learners to develop sets of competencies and literacies necessary to both understand the technology as well as their limitations and unexpected brittleness of such systems. In addition, a clear strategy within educational systems and a clear pedagogical approach with a strong focus on critical thinking and strategies for fact checking are required to integrate and take full advantage of large language models in learning settings and teaching curricula. Other challenges such as the potential bias in the output, the need for continuous human oversight, and the potential for misuse are not unique to the application of AI in education. But we believe that, if handled sensibly, these challenges can offer insights and opportunities in education scenarios to acquaint students early on with potential societal biases, criticalities, and risks of AI applications. We conclude with recommendations for how to address these challenges and ensure that such models are used in a responsible and ethical manner in education.},
  file         = {:ChatGPT-for-good-On-opportunities-and-challenges-of-large-language-models-for-education.pdf:PDF},
  keywords     = {Large language models, Artificial intelligence, Education, Educational technologies},
  priority     = {prio1},
}

@Article{Ye2023,
  author   = {Hongbin Ye and Tong Liu and Aijia Zhang and Wei Hua and Weiqiang Jia},
  date     = {2023-09-13},
  title    = {Cognitive Mirage: A Review of Hallucinations in Large Language Models},
  doi      = {https://doi.org/10.48550/arXiv.2309.06794},
  url      = {https://arxiv.org/abs/2309.06794},
  urldate  = {2025-11-30},
  file     = {:C\:/Users/milan/Downloads/Cognitive Mirage A Review of Hallucinations in Large Language Models.pdf:PDF},
  priority = {prio1},
}

@Article{Sadik2008,
  author       = {Sadik, Alaa},
  date         = {2008-04},
  journaltitle = {Educational Technology Research and Development},
  title        = {Digital storytelling: a meaningful technology-integrated approach for engaged student learning},
  doi          = {10.1007/s11423-008-9091-8},
  issn         = {1556-6501},
  number       = {4},
  pages        = {487--506},
  url          = {https://link.springer.com/article/10.1007/s11423-008-9091-8},
  urldate      = {2025-11-30},
  volume       = {56},
  file         = {:Digital storytelling a meaningful technology-integrated approach for engaged student learning.pdf:PDF},
  priority     = {prio1},
  publisher    = {Springer Science and Business Media LLC},
}

@InProceedings{Sarsa2022,
  author     = {Sarsa, Sami and Denny, Paul and Hellas, Arto and Leinonen, Juho},
  booktitle  = {Proceedings of the 2022 ACM Conference on International Computing Education Research - Volume 1},
  date       = {2022-08},
  title      = {Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models},
  doi        = {10.1145/3501385.3543957},
  pages      = {27--43},
  publisher  = {ACM},
  series     = {ICER 2022},
  url        = {https://dl.acm.org/doi/10.1145/3501385.3543957},
  urldate    = {2025-11-30},
  collection = {ICER 2022},
  file       = {:Automatic generation of programming exercises and code explanations using large language models.pdf:PDF},
  priority   = {prio1},
}

@Article{Messer2024,
  author       = {Messer, Marcus and Brown, Neil C. C. and Kölling, Michael and Shi, Miaojing},
  date         = {2024-02},
  journaltitle = {ACM Transactions on Computing Education},
  title        = {Automated Grading and Feedback Tools for Programming Education: A Systematic Review},
  doi          = {10.1145/3636515},
  issn         = {1946-6226},
  number       = {1},
  pages        = {1--43},
  url          = {https://dl.acm.org/doi/10.1145/3636515},
  urldate      = {2025-11-30},
  volume       = {24},
  file         = {:Automated Grading and Feedback Tools for Programming.pdf:PDF},
  priority     = {prio1},
  publisher    = {Association for Computing Machinery (ACM)},
}

@Misc{Zhou2025,
  author    = {Zhou, Yiqiu and Pankiewicz, Maciej and Paquette, Luc and Baker, Ryan},
  date      = {2025},
  editor    = {Jiang and B. et al},
  title     = {Impact of LLM Feedback on Learner Persistence in Programming},
  url       = {https://learninganalytics.upenn.edu/ryanbaker/ICCE2025_paper_28.pdf},
  urldate   = {2025-11-30},
  abstract  = {This study examines how Large Language Model (LLM) feedback generated for compiler errors impacts learners' persistence in programming tasks within a system for automated assessment of programming assignments. Persistence, the ability to maintain effort in the face of challenges, is crucial for academic success but can sometimes lead to unproductive "wheel spinning" when students struggle without progress. We investigated how additional LLM feedback based on the GPT-4 model, provided for compiler errors affects learners' persistence within a CS1 course. Specifically, we examined whether its impacts differ based on task difficulty, and if the effects persist after the feedback is removed. A randomized controlled trial involving 257 students across various programming tasks was conducted. Our findings reveal that LLM feedback improved some aspects of students' performance and persistence, such as increased scores, a higher likelihood of solving problems, and a lower tendency to demonstrate unproductive "wheel spinning" behavior. Notably, this positive impact was also observed in challenging tasks. However, its benefits did not sustain once the feedback was removed. The results highlight both the potential and limitations of LLM feedback, pointing out the need to promote long-term skill development and learning independent of immediate AI assistance.},
  booktitle = {Jiang, B. et al. (Eds.) (2025). Proceedings of the 33rd International Conference on Computers in Education.},
  file      = {:Impact of LLM Feedback on Learner  Persistence in Programming.pdf:PDF},
  keywords  = {Autograding, Automated feedback, Programming, Persistence, GPT, LLM},
  priority  = {prio1},
  publisher = {Springer},
  year      = {2025},
}

@Misc{Ȃdurean2025,
  author   = {Ȃdurean, Victor-Alexandru and Phung, Tung and Kotalwar, Nachiket and Liut, Michael and Leinonen, Juho and Denny, Paul},
  date     = {2025},
  title    = {Humanizing Automated Programming Feedback: Fine-Tuning Generative Models with Student-Written Feedback},
  doi      = {10.5281/zenodo.15870290},
  url      = {https://juholeinonen.com/assets/pdf/padurean2025humanizing.pdf},
  urldate  = {2025-11-30},
  abstract = {The growing need for automated and personalized feedback in programming education has led to recent interest in leveraging generative AI for feedback generation. However, current approaches tend to rely on prompt engineering techniques in which predefined prompts guide the AI to generate feedback. This can result in rigid and constrained responses that fail to accommodate the diverse needs of students and do not reflect the style of human-written feedback from tutors or peers. In this study, we explore learnersourcing as a means to fine-tune language models for generating feedback that is more similar to that written by humans, particularly peer students. Specifically, we asked students to act in the flipped role of a tutor and write feedback on programs containing bugs. We collected approximately 1,900 instances of student-written feedback on multiple programming problems and buggy programs. To establish a baseline for comparison, we analyzed a sample of $300$ instances based on correctness, length, and how the bugs are described. Using this data, we fine-tuned open-access generative models, specifically Llama3 and Phi3. Our findings indicate that fine-tuning models on learnersourced data not only produces feedback that better matches the style of feedback written by students, but also improves accuracy compared to feedback generated through prompt engineering alone, even though some student-written feedback is incorrect. This surprising finding highlights the potential of student-centered fine-tuning to improve automated feedback systems in programming education.},
  file     = {:Humanizing Automated Programming Feedback Fine-Tuning Generative Models with Student-Written Feedback.pdf:PDF},
  keywords = {programming feedback, fine-tuning, generative AI},
  priority = {prio1},
  year     = {2025},
}

@Misc{Koutcheme2025,
  author   = {Koutcheme, Charles and Dainese, Nicola and Sarsa, Sami and Hellas, Arto and Leinonen, Juho and Ashraf, Syed and Denny, Paul},
  date     = {2025-03-01},
  title    = {Evaluating Language Models for Generating and Judging Programming Feedback},
  doi      = {10.1145/3641554.3701791},
  url      = {https://juholeinonen.com/assets/pdf/koutcheme2025evaluating.pdf},
  urldate  = {2025-11-30},
  abstract = {The emergence of large language models (LLMs) has transformed research and practice across a wide range of domains. Within the computing education research (CER) domain, LLMs have garnered significant attention, particularly in the context of learning programming. Much of the work on LLMs in CER, however, has focused on applying and evaluating proprietary models. In this article, we evaluate the efficiency of open-source LLMs in generating high-quality feedback for programming assignments and judging the quality of programming feedback, contrasting the results with proprietary models. Our evaluations on a dataset of students' submissions to introductory Python programming exercises suggest that state-of-the-art open-source LLMs are nearly on par with proprietary models in both generating and assessing programming feedback. Additionally, we demonstrate the efficiency of smaller LLMs in these tasks and highlight the wide range of LLMs accessible, even for free, to educators and practitioners. CCS Concepts • Social and professional topics → Computing education.},
  file     = {:Evaluating Language Models for Generating and Judging Programming Feedback.pdf:PDF},
  keywords = {open source, large language models, generative AI, automatic feedback, automatic evaluation, programming feedback, LLM-as-a-judge},
  number   = {provides},
  priority = {prio1},
  year     = {2025},
}

@Comment{jabref-meta: databaseType:biblatex;}
