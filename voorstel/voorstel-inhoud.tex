%===================
%    INLEIDING
%===================
\section{Inleiding}%
\label{sec:inleiding}

De afgelopen jaren hebben Large Language Models (LLM's) zoals ChatGPT, Gemini, Claude, \newline DeepSeek en GitHub Copilot een enorme impact gecreëerd op bijna alles en iedereen. AI (Artificiële Intelligentie) valt bij velen niet meer weg te denken uit het dagelijks leven, en wordt elke dag opnieuw door ontelbare aantallen mensen ingezet om hun taken te vergemakkelijken. Zo ook door studenten Full Stack Development binnen de opleiding Toegepaste Informatica aan HOGENT. Deze studenten zetten LLM's steeds vaker in om code te begrijpen, debuggen, zelfs schrijven.

\vspace{1em}

Maar hoe goed zijn die LLM's in het begrijpen van code? Hoe goed is de feedback die ze genereren? Is deze wel correct? Is deze bruikbaar? Dat zijn allemaal vragen waar de studenten zelf weinig bij stil staan, maar dit is wel relevant. Binnen de opleidingsonderdelen Front-End Web Development en Web Services worden er namelijks per semester tientallen projecten ingediend, en deze worden momenteel nog volledig handmatig beoordeeld door de lectoren. Die manuele beoordeling is enorm tijdsintensief, en repetitief. Lectoren moeten dezelfde criteria, denk bijvoorbeeld aan structuur, consistentie, documentatie, foutafhandeling, testing,\ldots telkens opnieuw toepassen op elk project. Hierdoor gaat eigenlijk een groot deel van hun tijd naar het puur controleren van de aanwezigheid van bepaalde onderdelen in het project, en niet naar het geven van inhoudelijke feedback. Verder is het ook lastig voor de lectoren om consistent feedback te geven. Zo hangt de feedback die een bepaald project zou krijgen ook nog eens af van welke persoon het beoordeeld.

\vspace{1em}

Deze concrete probleemsituatie geeft aanleiding tot de vraag of lectoren LLM's niet zouden kunnen gebruiken ter ondersteuning, door feedback te genereren over de code-kwaliteit van een studentenproject. Hoe bruikbaar, betrouwbaar, consistent of accuraat deze feedback is, en hoe consistent ze is in vergelijking met de feedback van een lector is echter nog onduidelijk.

\vspace{1em}

Daaruit vloeit dus ook volgende onderzoeksvraag voort:

\begin{quote}
    \textit{``In welke mate kunnen Large Language Models bruikbare en betrouwbare formatieve feedback genereren op code-kwaliteit van studentenprojecten binnen webontwikkeling, en hoe verhouden deze beoordelingen zich tot de evaluaties van ervaren lectoren?''}
\end{quote}

\vspace{1em}

Om deze onderzoeksvraag te beantwoorden worden onderstaande deelvragen onderzocht. \newline Deze hebben enerzijds betrekking tot het probleemdomein, anderzijds tot het oplossingsdomein.

\vspace{1em}

\textbf{\newline Deelvragen probleemdomein:}
\begin{enumerate}
    \item Welke specifieke code-criteria worden momenteel gebruikt bij de evaluatie van projecten binnen Front-end Web Development en Web Services?
    \item Hoeveel tijd wordt gemiddeld gespendeerd aan het beoordelen van 1 project?
    \item Van de tijd die gespendeerd wordt aan het beoordelen van 1 project, welk deel is gericht op het controleren van de criteria, en welk deel is gericht op het geven van inhoudelijke feedback?
    \item In welke mate ervaren de lectoren inconsistenties in hun eigen beoordelingen ten opzichte van hun collega's?
\end{enumerate}

\vspace{1em}

\textbf{Deelvragen oplossingsdomein:}
\begin{enumerate}
    \item Hoe goed zijn verschillende LLM's in het detecteren van vooraf gedefinieerde criteria in de codebase van zo'n studentenproject?
    \item In welke mate komen de feedback van lectoren en LLM's overeen?
    \item Hoe consistent is de feedback van verschillende LLM's bij hetzelfde project?
    \item Wat zijn de sterktes van de door een LLM gegenereerde feedback over zo'n studentenproject?
    \item Wat zijn de zwaktes van de door een LLM gegenereerde feedback over zo'n studentenproject?
    \item Wat is de potentiële tijdswinst bij het gebruik van een LLM, ten opzichte van alle projecten handmatig evalueren?
\end{enumerate}

\vspace{1em}

Het doel van dit onderzoek is dus om te gaan kijken in hoeverre LLM's gebruikt kunnen worden ter ondersteuning bij het evalueren van studentenprojecten. Naast de scriptie omvat het eindresultaat ook een Proof-Of-Concept (POC) testomgeving waarmee LLM's systematisch beoordeeld zullen worden op o.a. hun vermogen om criteria te herkennen en consistente feedback te genereren. De doelgroep van dit onderzoek bestaat uit de lectoren Front-End Web Development en Web Services aan HOGENT.

\section{Literatuurstudie}%
\label{sec:literatuurstudie}

\paragraph{De evolutie van automatische beoordelingen binnen het programmeer onderwijs}
Automatische beoordelingen, ook wel gekend als autograding, wordt steeds belangrijker binnen programmeer onderwijs. Deze systemen richten zich voornamelijk op correcte syntax en de resultaten bij het uitvoeren van de code, waardoor de focus meer op kwantitatieve beoordeling ligt, in de plaats van kwalitatieve feedback geven \autocite{Messer2024}. Hoewel deze systemen de docenten dus wel ontlasten, blijven ze echter beperkt in het bieden van gedetailleerde uitleg, en zijn ze ook minder goed in het detecteren an creatievere of alternatieve oplossingen. 

\paragraph{De opkomst van LLM's binnen het onderwijs}
De introductie van LLM's zoals ChatGPT en Gemini biedt dan weer veel meer mogelijkheden voor het automatiseren van feedback en beoordelingen. Niet alleen kan een LLM syntax fouten detecteren. Het kan ook de context snappen van een bepaalde implementatie, en hier suggesties of verbeteringen voor geven, iets wat traditionele autograding tools niet kunnen \autocite{Kasneci2023}. \textcite{Ye2023} waarschuwt echter voor hallicunaties, waarbij een model foutieve of inconsistente feedback genereert. Ondanks het risico hierop tonen vele studies aan dat LLM's waardevolle ondersteuning kunnen bieden, maar enkel wanneer ze gepaard gaan met menselijk toezicht, en heel duidelijke pedagogische richtlijnen \autocite{Kasneci2023}.

\paragraph{LLM's voor het automaisch genereren van feedback en beoordelen van code-kwaliteit}
Recent onderzoek toont aan dat het verschil tussen LLM's en menselijke feedback steeds kleiner wordt. Zo vergelijkt \textcite{Mendonca2025} een open-source model (LLaMA 3.2) met een betalend model (GPT-4o) en mensen. Uit het onderzoek bleek dat het betalend model praktisch dezelfde scores levert voor vragen over code. Ook \textcite{Koutcheme2025} onderzocht open-source LLM's, en toonde aan dat deze efficiënt en bijna even accuraat feedback kunnen genereren en beoordelen als eigen modellen. Verder benadrukte \textcite{Adurean2025} dat fine-tuning samen met student-gegenereerde feedback de stijl en precisie van AI gegenereerde feedback verder verbetert. Hierdoor komt deze nog dichter bij menselijke evaluaties.

\paragraph{De impact op studenten en hun leerproces}
Het gebruiken van door LLM gegenereerde feedback heeft invloed op zowel de prestaties als het doorzettingsvermogen van studenten. \textcite{Zhou2025} toont in een RCT (Randomized Controlled Trial) aan dat door LLM gegenereerde feedback de kans zou vergroten dat studenten problemen oplossen, dat ze hogere scores zouden behalen en dat ze minder onproductief ``wheel-spinning'' vertonen. Deze effecten waren zichtbaar bij zowel eenvoudige als complexe programmeertaken, en verdwenen van zodra de feedback werd verwijderd. \textcite{Lepp2025} onderzocht dan weer de studentenpercepties, en rapporteerde dat studenten LLM's vooral gebruiken voor het begrijpen van code, en voor het oplossen van problemen. Dit heeft een positieve invloed op hun leerervaring, maar uiteraard vormt overmatige afhankelijkheid van AI wel een risico.

\paragraph{De uitdagingen die gepaard gaan met LLM's}
Ondanks de vele voordelen brengen LLM's ook nadelen en uitdagingen met zich mee. Zo benadrukt \textcite{Ye2023} de aanwezigheid van hallucinaties en een bias in LLM-output. \textcite{Gotoman2025} onderzocht dan weer detectietools voor teksten gegenereerd door AI en concludeerde dat de betrouwbaarheid van deze tools nog variabel is. Menselijk toezicht blijft essentieel. Als laatste benadrukte \textcite{Kasneci2023} dat de combinatie van pedagogische strategieën en kennis van AI noodzakelijk is om zowel studenten als docenten bewust te maken van de beperkingen.

\paragraph{Onderzoeksrelevantie}
Hoewel LLM's steeds vaker onderzocht worden, ook binnen programmeer onderwijs, is het duidelijk dat er verder onderzoek moet worden gedaan naar hoe lectoren LLM's efficiënt kunnen inzetten ter ondersteuning bij het beoordelen van studentenprojecten. Bestaande studies richten zich namelijk vooral op autograding voor individuele programmeer oefeningen. Daardoor blijft het onduidelijk in hoeverre door LLM gegenereerde feedback bij kan dragen aan consistente evaluaties, de werkdruk en efficiëntie van lectoren kan verbeteren, zeker binnen zeer uiteenlopende studentenprojecten. Het verder verkennen van deze toepassing is essentieel om te bepalen of LLM's daadwerkelijk ingezet kunnen worden voor deze doeleinden.

\section{Methodologie}%
\label{sec:methodologie}

%===================
%    INLEIDENDE 
%      ALINEA
%===================
Deze bachelorproef zal uitgewerkt worden in verschillende fases. In elk van deze fases wordt een andere techniek gebruikt, en wordt de focus op een ander onderdeel van het probleem gelegd. In totaal zijn er 14 werkdagen voorzien voor de bachelorproef. Aangezien er 1 werkdag per week besteed wordt aan het onderzoek, komt 1 werkdag overeen met 1 week.

Een overzicht van de fasen wordt weergegeven in Figuur~\ref{fig:gantt}.

%===================
%      FASE 1
%===================
\paragraph{Fase 1: Probleemdomein onderzoeken (1 werkdag)}
In de eerste fase van dit onderzoek zal het probleemdomein verder worden ontdekt. Hiervoor zal er een interview worden afgenomen met een lector Web Services & Front-end Web development. Dit is namelijk de primaire doelgroep van deze bachelorproef. Dit interview wordt gedaan voor 3 redenen: 
\begin{itemize}
    \item Het verder verdiepen in het probleem en de evaluatiecriteria en methoden die de lectoren vandaag hanteren.
    \item De nood aan onderzoek, en de relevantie van het onderzoek verder te verduidelijken.
    \item Nagaan of er studentenprojecten van verschillende kwaliteitsniveaus kunnen worden opgeleverd. Dit zou bij voorkeur ook door verschillende lectoren gedaan worden, om de consistentie van de evaluaties in acht te kunnen nemen.
\end{itemize}

Het resultaat van deze fase is een duidelijker inzicht binnen het probleemdomein, een lijst met de verschillende evaluatiecriteria, alsook verschillende studentenprojecten die gebruikt kunnen worden als testprojecten in een volgende fase.

%===================
%      FASE 2
%===================
\paragraph{Fase 2: Literatuurstudie (2 werkdagen)}
De tweede fase bevat de literatuurstudie. Hierin zal er gekeken worden naar welke LLMs (Large Languague Models) er het meest geschikt zijn voor het analyseren van code-kwaliteit, en het geven van consistente en duidelijke feedback. Deze literatuurstudie zal zowel lokale als cloud modellen onderzoeken, via Ollama. Hieruit zal een longlist worden opgesteld met de potentiële modellen die in aanmerking komen voor verder onderzoek. Bij het kiezen van de modellen wordt er aandacht besteed aan:
\begin{itemize}
    \item Voordelen
    \item Nadelen
    \item Token- en contextlimitaties
    \item Geschiktheid voor code-analyse
\end{itemize}
Als laatste zal er ook gekeken worden naar hoe de te grote projecten zullen worden opgesplitst, hier bestaan er namelijk ook verschillende manieren voor. Denk hierbij aan per bestand, per laag, via chunking, via een samenvatting,\ldots

\noindent Het resultaat van deze fase is een duidelijke, goed onderbouwde lijst van bruikbare modellen, inclusief hun technische eigenschappen en eventuele beperkingen.

%===================
%      FASE 3
%===================
\paragraph{Fase 3: Opzetten testomgeving (4 werkdagen)}
Tijdens de derde fase zal de testomgeving worden opgezet. Dit omvat het opzetten van een script dat de studentenprojecten opsplitst om te voldoen aan de token limieten van het model, alsook het opzetten van een script dat de gesplitste delen automatisch verstuurt naar verschillende modellen met een identieke prompt. Binnen deze fase zal er ook gekeken worden om eventueel samen werken met het VIC (Virtual IT Company) van HOGENT. Hier is het namelijk misschien mogelijk om sterkere servers te gebruiken om de rekenkracht van een model te verhogen, iets wat voor dit onderzoek wel van pas zou kunnen komen. In deze fase is het zeer belangrijk dat alles goed gedocumenteerd wordt, zodat het onderzoek reproduceerbaar en herbruikbaar blijft.

Het resultaat van deze fase is een volledig werkende proof-of-concept die klaar is om de vergelijkende analyse te gaan uitvoeren. Dit, inclusief de documentatie, zou beschikbaar moeten zijn op een GitHub repository.

%===================
%      FASE 4
%===================
\paragraph{Fase 4: Kwantitatieve evaluatie van LLMs (5 werkdagen)} 
De vierde fase zal het individueel evalueren van verschillende LLMs omvatten. Op basis van de literatuurstudie zullen de meest relevante en bruikbare modellen worden gekozen, en deze zullen individueel worden beoordeeld. Hierbij zal gekeken worden naar 4 punten:
\begin{itemize}
    \item Het detecteren van evaluatiecriteria.
    \item Klopt de output?
    \item Is de output consistent met die van leerkrachten?
    \item Is de output consistent wanneer hetzelfde project meerdere keren geëvalueerd wordt?
    \item Hoeveel potentiële tijdswinst zit er verbonden aan LLM ondersteunde code-evaluatie?
\end{itemize}

Wanneer alle modellen individueel zijn beoordeeld, dan volgt de vergelijkende studie. Hierin worden de verschillende modellen met elkaar vergeleken op alle bovenstaande criteria.

Het resultaat van deze fase is een uitgebreide analyse en een grondige vergelijking op basis van de bekomen resultaten.

%===================
%      FASE 5
%===================
\paragraph{Fase 5: Conclusie (2 werkdagen)}
In de laatste fase zal de conclusie worden opgebouwd. De resultaten van al het onderzoek uit voorgaande fases worden gebundeld om objectief aan te tonen of LLMs mogelijks gebruikt kunnen worden voor het evalueren van studentenprojecten binnen het afgebakende probleemdomein. Daarnaast worden de verschillende voor- en nadelen van de geëvalueerde modellen toegelicht, om de keuze voor lectoren te vergemakkelijken.

Het resultaat van deze fase is een volledig uitgewerkte conclusie, aangevuld met de bevindingen, aanbevelingen voor lectoren en eventuele suggesties voor verder onderzoek.

\section{Verwacht resultaat, conclusie}%
\label{sec:verwachte_resultaten}

Op basis van de vooraf geplande POC testomgeving en de vergelijkingen tussen de feedback van lectoren en LLM's wordt er verwacht dat het onderzoek geen simpele oplossing zal opleveren. Niet 1 LLM zal de allerbeste zijn. De gebruikte LLM's zullen waarschijnlijk sterk presteren in het herkennen van duidelijke criteria zoals documentatie, structuur, foutafhandeling en de aanwezigheid van testen. Voor meer inhoudelijke of contextgebonden feedback wordt er minder consistentie verwacht. De opmerkingen van LLM's hier zullen waarschijnlijk algemener zijn, en beperkt toepasbaar.

\vspace{1em}

Hiernaast wordt er ook vermoed dat de feedback van 1 bepaald model relatief consistent zou blijven over meerdere analyses van eenzelfde project. Tussen verschillende modellen zullen er naar verwachting wel merkbare verschillen naar boven komen. Denk aan niveau van detail, schrijfstijl, correctheid en strengheid. In de vergelijking met de feedback van lectoren zal vermoedelijk blijken dat LLM's de controles op basiscriteria sneller kunnen uitvoeren, maar dat ze nog niet in staat zijn om de complexere projecten volledig accuraat en betrouwbaar te beoordelen.

\vspace{1em}

De potentiële tijdswinst vormt voor de lectoren waarschijnlijk een van de grootste meerwaarden. Om verschillende basis controles, die voor elk project hetzelfde zijn, steeds opnieuw te moeten uitvoeren is zeer repetitief. Als deze automatisch zouden kunnen verlopen, ontstaat er voor hun meer ruimte voor inhoudelijke feedback. Dit betekent niet dat LLM's in staat zijn om een volledig project te gaan beoordelen, maar wel dat ze als ondersteuning toch een deel van de werkdruk aanzienlijk kunnen verlagen. Uiteraard moet de LLM gegenereerde feedback dan wel nog steeds worden nagekeken.

\vspace{1em}

Over het algemeen wordt er verwacht dat dit onderzoek zal aantonen dat LLM's bruikbaar zijn ter ondersteuning binnen de evaluatie van studentenprojecten voor de vakken Front-End Web Development en Web Services aan HOGENT. Voornamelijk het detecteren van aanwezige criteria binnen het project lijkt handig en voor een enorme tijdswinst te zorgen voor de lectoren. De uiteindelijke inhoudelijke evaluatie blijft waarschijnlijk best nog even bij de lectoren zelf, omdat hier LLM's meer moeite mee hebben. In hoeverre LLM's zullen evolueren de komende tijd is echter onbekend, waardoor dit in de toekomst zeker nog zou kunnen veranderen.